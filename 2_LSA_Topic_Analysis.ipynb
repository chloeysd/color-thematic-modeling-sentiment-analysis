{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yinshuodi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yinshuodi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yinshuodi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Analysis for red documents:\n",
      "___topic 0___\n",
      "maya: 0.09\n",
      "culture: 0.10\n",
      "sport: 0.10\n",
      "renaissance: 0.10\n",
      "laser: 0.11\n",
      "emotion: 0.12\n",
      "scarlet: 0.13\n",
      "war: 0.14\n",
      "china: 0.15\n",
      "roman: 0.15\n",
      "___topic 1___\n",
      "behavior: 0.08\n",
      "rgb: 0.08\n",
      "brand: 0.08\n",
      "emotional: 0.10\n",
      "commentator: 0.10\n",
      "athlete: 0.15\n",
      "sport: 0.19\n",
      "psychology: 0.20\n",
      "anger: 0.28\n",
      "emotion: 0.39\n",
      "___topic 2___\n",
      "dwarf: 0.13\n",
      "spot: 0.14\n",
      "sunrise: 0.15\n",
      "cmyk: 0.15\n",
      "wavelength: 0.15\n",
      "mar: 0.15\n",
      "opsin: 0.16\n",
      "ink: 0.17\n",
      "laser: 0.24\n",
      "rgb: 0.24\n",
      "___topic 3___\n",
      "pigment: 0.09\n",
      "vermilion: 0.09\n",
      "rgb: 0.09\n",
      "football: 0.11\n",
      "commentator: 0.12\n",
      "race: 0.12\n",
      "sport: 0.13\n",
      "ferrari: 0.13\n",
      "laser: 0.16\n",
      "scarlet: 0.32\n",
      "___topic 4___\n",
      "ink: 0.09\n",
      "painter: 0.10\n",
      "rgb: 0.11\n",
      "painting: 0.11\n",
      "anger: 0.12\n",
      "vermilion: 0.14\n",
      "artist: 0.16\n",
      "emotion: 0.16\n",
      "renaissance: 0.20\n",
      "scarlet: 0.20\n",
      "\n",
      "\n",
      "LSA Analysis for orange documents:\n",
      "___topic 0___\n",
      "produce: 0.09\n",
      "spice: 0.10\n",
      "orpiment: 0.10\n",
      "egyptian: 0.11\n",
      "pumpkin: 0.11\n",
      "india: 0.11\n",
      "national: 0.13\n",
      "carrot: 0.13\n",
      "rgb: 0.14\n",
      "pigment: 0.15\n",
      "___topic 1___\n",
      "mixture: 0.09\n",
      "combination: 0.10\n",
      "boldface: 0.11\n",
      "hue: 0.12\n",
      "produce: 0.12\n",
      "equal: 0.12\n",
      "mix: 0.15\n",
      "pigment: 0.16\n",
      "secondary: 0.20\n",
      "rgb: 0.29\n",
      "___topic 2___\n",
      "tree: 0.10\n",
      "seed: 0.10\n",
      "spice: 0.10\n",
      "chlorophyll: 0.14\n",
      "pigment: 0.15\n",
      "leaf: 0.16\n",
      "autumn: 0.16\n",
      "orpiment: 0.18\n",
      "pumpkin: 0.32\n",
      "carrot: 0.37\n",
      "___topic 3___\n",
      "arche: 0.10\n",
      "flagpole: 0.10\n",
      "rgb: 0.10\n",
      "safety: 0.13\n",
      "pumpkin: 0.14\n",
      "park: 0.14\n",
      "netherland: 0.14\n",
      "organisation: 0.14\n",
      "nation: 0.16\n",
      "national: 0.32\n",
      "___topic 4___\n",
      "inmate: 0.07\n",
      "study: 0.08\n",
      "brand: 0.08\n",
      "participant: 0.09\n",
      "product: 0.10\n",
      "preference: 0.11\n",
      "prison: 0.15\n",
      "prisoner: 0.16\n",
      "halloween: 0.18\n",
      "pumpkin: 0.22\n",
      "\n",
      "\n",
      "LSA Analysis for yellow documents:\n",
      "___topic 0___\n",
      "egypt: 0.09\n",
      "primarie: 0.09\n",
      "byzantine: 0.10\n",
      "tour: 0.10\n",
      "wavelength: 0.10\n",
      "ochre: 0.11\n",
      "rgb: 0.12\n",
      "pigment: 0.13\n",
      "gold: 0.17\n",
      "china: 0.17\n",
      "___topic 1___\n",
      "finish: 0.08\n",
      "desgrange: 0.09\n",
      "winner: 0.11\n",
      "classification: 0.12\n",
      "france: 0.17\n",
      "stage: 0.17\n",
      "race: 0.24\n",
      "rider: 0.36\n",
      "jersey: 0.41\n",
      "tour: 0.58\n",
      "___topic 2___\n",
      "period: 0.09\n",
      "vega: 0.09\n",
      "trade: 0.09\n",
      "chinese: 0.10\n",
      "asia: 0.11\n",
      "egyptian: 0.11\n",
      "empire: 0.14\n",
      "egypt: 0.14\n",
      "byzantine: 0.17\n",
      "china: 0.24\n",
      "___topic 3___\n",
      "ray: 0.09\n",
      "study: 0.09\n",
      "object: 0.10\n",
      "theory: 0.12\n",
      "sunlight: 0.13\n",
      "wave: 0.13\n",
      "pleasure: 0.13\n",
      "optimism: 0.14\n",
      "solar: 0.16\n",
      "radiation: 0.24\n",
      "___topic 4___\n",
      "solar: 0.14\n",
      "plant: 0.14\n",
      "yolk: 0.14\n",
      "egg: 0.15\n",
      "flower: 0.15\n",
      "radiation: 0.17\n",
      "maize: 0.19\n",
      "buttercup: 0.22\n",
      "ranunculu: 0.25\n",
      "specie: 0.27\n",
      "\n",
      "\n",
      "LSA Analysis for green documents:\n",
      "___topic 0___\n",
      "health: 0.09\n",
      "organism: 0.09\n",
      "cell: 0.11\n",
      "emotion: 0.11\n",
      "emerald: 0.11\n",
      "pigment: 0.11\n",
      "earth: 0.11\n",
      "chlorophyll: 0.14\n",
      "life: 0.14\n",
      "plant: 0.25\n",
      "___topic 1___\n",
      "earth: 0.11\n",
      "photosynthetic: 0.12\n",
      "life: 0.12\n",
      "organism: 0.12\n",
      "electron: 0.13\n",
      "photosynthesi: 0.14\n",
      "cell: 0.14\n",
      "carbon: 0.15\n",
      "chlorophyll: 0.29\n",
      "plant: 0.47\n",
      "___topic 2___\n",
      "rifle: 0.11\n",
      "subtractive: 0.11\n",
      "magenta: 0.12\n",
      "chlorophyll: 0.12\n",
      "ryb: 0.14\n",
      "mix: 0.15\n",
      "pigment: 0.17\n",
      "rgb: 0.18\n",
      "primarie: 0.19\n",
      "emerald: 0.24\n",
      "___topic 3___\n",
      "preference: 0.07\n",
      "mix: 0.07\n",
      "emotional: 0.07\n",
      "theory: 0.07\n",
      "ryb: 0.08\n",
      "primarie: 0.10\n",
      "hope: 0.11\n",
      "health: 0.14\n",
      "envy: 0.24\n",
      "emotion: 0.40\n",
      "___topic 4___\n",
      "gem: 0.07\n",
      "english: 0.07\n",
      "hope: 0.08\n",
      "youth: 0.10\n",
      "shade: 0.10\n",
      "rifle: 0.14\n",
      "health: 0.14\n",
      "envy: 0.16\n",
      "emotion: 0.17\n",
      "emerald: 0.50\n",
      "\n",
      "\n",
      "LSA Analysis for cyan documents:\n",
      "___topic 0___\n",
      "earth: 0.11\n",
      "printing: 0.12\n",
      "temperature: 0.12\n",
      "flame: 0.13\n",
      "uranu: 0.13\n",
      "atmosphere: 0.13\n",
      "cmyk: 0.13\n",
      "natural: 0.14\n",
      "ink: 0.21\n",
      "water: 0.28\n",
      "___topic 1___\n",
      "printer: 0.11\n",
      "turquoise: 0.12\n",
      "model: 0.12\n",
      "subtractive: 0.13\n",
      "teal: 0.17\n",
      "magenta: 0.17\n",
      "printing: 0.23\n",
      "rgb: 0.23\n",
      "cmyk: 0.29\n",
      "ink: 0.45\n",
      "___topic 2___\n",
      "feminist: 0.09\n",
      "cinecolor: 0.09\n",
      "dialect: 0.11\n",
      "object: 0.11\n",
      "lapi: 0.11\n",
      "historian: 0.13\n",
      "prussian: 0.14\n",
      "artist: 0.16\n",
      "greek: 0.18\n",
      "cyanotype: 0.23\n",
      "___topic 3___\n",
      "fuel: 0.08\n",
      "bacteria: 0.08\n",
      "cooktop: 0.08\n",
      "oven: 0.11\n",
      "burner: 0.17\n",
      "food: 0.17\n",
      "electric: 0.17\n",
      "flame: 0.27\n",
      "cooking: 0.27\n",
      "stove: 0.37\n",
      "___topic 4___\n",
      "wikipedia: 0.11\n",
      "stub: 0.11\n",
      "muscovite: 0.11\n",
      "brooch: 0.11\n",
      "granite: 0.11\n",
      "english: 0.11\n",
      "gemstone: 0.12\n",
      "turquoise: 0.22\n",
      "aquamarine: 0.51\n",
      "teal: 0.52\n",
      "\n",
      "\n",
      "LSA Analysis for blue documents:\n",
      "___topic 0___\n",
      "particle: 0.10\n",
      "cobalt: 0.10\n",
      "azure: 0.11\n",
      "sea: 0.11\n",
      "wavelength: 0.13\n",
      "violet: 0.14\n",
      "ultramarine: 0.14\n",
      "pigment: 0.15\n",
      "scatter: 0.19\n",
      "sky: 0.23\n",
      "___topic 1___\n",
      "cloud: 0.08\n",
      "celeste: 0.10\n",
      "sun: 0.10\n",
      "scattering: 0.12\n",
      "wavelength: 0.14\n",
      "rayleigh: 0.15\n",
      "tyndall: 0.18\n",
      "particle: 0.21\n",
      "sky: 0.39\n",
      "scatter: 0.41\n",
      "___topic 2___\n",
      "painting: 0.09\n",
      "indigo: 0.11\n",
      "dye: 0.12\n",
      "lazuli: 0.15\n",
      "cobalt: 0.16\n",
      "lapi: 0.17\n",
      "azure: 0.22\n",
      "violet: 0.25\n",
      "pigment: 0.28\n",
      "ultramarine: 0.38\n",
      "___topic 3___\n",
      "italian: 0.07\n",
      "sunset: 0.07\n",
      "displayed: 0.08\n",
      "horizon: 0.09\n",
      "sun: 0.09\n",
      "halo: 0.10\n",
      "shade: 0.11\n",
      "celeste: 0.21\n",
      "azure: 0.24\n",
      "sky: 0.47\n",
      "___topic 4___\n",
      "perovskite: 0.09\n",
      "value: 0.09\n",
      "display: 0.11\n",
      "video: 0.11\n",
      "bit: 0.11\n",
      "phosphor: 0.12\n",
      "image: 0.14\n",
      "primarie: 0.18\n",
      "device: 0.19\n",
      "rgb: 0.24\n",
      "\n",
      "\n",
      "LSA Analysis for purple documents:\n",
      "___topic 0___\n",
      "china: 0.08\n",
      "roman: 0.08\n",
      "wear: 0.09\n",
      "murex: 0.09\n",
      "snail: 0.10\n",
      "manganese: 0.12\n",
      "pigment: 0.14\n",
      "tyrian: 0.17\n",
      "dye: 0.46\n",
      "violet: 0.48\n",
      "___topic 1___\n",
      "million: 0.08\n",
      "bcci: 0.10\n",
      "franchise: 0.10\n",
      "fremantle: 0.13\n",
      "raven: 0.15\n",
      "china: 0.15\n",
      "league: 0.18\n",
      "player: 0.18\n",
      "nba: 0.19\n",
      "season: 0.24\n",
      "___topic 2___\n",
      "impressionist: 0.07\n",
      "nba: 0.07\n",
      "artist: 0.07\n",
      "monet: 0.07\n",
      "spectral: 0.08\n",
      "cobalt: 0.08\n",
      "season: 0.08\n",
      "pigment: 0.12\n",
      "manganese: 0.24\n",
      "violet: 0.56\n",
      "___topic 3___\n",
      "baltimore: 0.11\n",
      "bcci: 0.11\n",
      "franchise: 0.12\n",
      "fremantle: 0.16\n",
      "player: 0.18\n",
      "dye: 0.19\n",
      "raven: 0.20\n",
      "league: 0.20\n",
      "nba: 0.23\n",
      "season: 0.29\n",
      "___topic 4___\n",
      "blackberrie: 0.10\n",
      "neck: 0.10\n",
      "specie: 0.10\n",
      "reed: 0.10\n",
      "silk: 0.11\n",
      "nest: 0.13\n",
      "china: 0.14\n",
      "bird: 0.23\n",
      "heron: 0.28\n",
      "finch: 0.30\n",
      "\n",
      "\n",
      "LSA Analysis for black documents:\n",
      "___topic 0___\n",
      "witchcraft: 0.10\n",
      "darknes: 0.11\n",
      "period: 0.11\n",
      "enlightenment: 0.12\n",
      "empire: 0.12\n",
      "death: 0.14\n",
      "uv: 0.14\n",
      "age: 0.15\n",
      "magic: 0.15\n",
      "roman: 0.15\n",
      "___topic 1___\n",
      "emit: 0.11\n",
      "fluorescent: 0.11\n",
      "wave: 0.11\n",
      "visible: 0.17\n",
      "wavelength: 0.17\n",
      "lamp: 0.18\n",
      "ultraviolet: 0.19\n",
      "infrared: 0.21\n",
      "radiation: 0.27\n",
      "uv: 0.50\n",
      "___topic 2___\n",
      "religion: 0.10\n",
      "practice: 0.11\n",
      "belief: 0.11\n",
      "uv: 0.13\n",
      "witche: 0.14\n",
      "witch: 0.14\n",
      "magical: 0.15\n",
      "evil: 0.18\n",
      "witchcraft: 0.38\n",
      "magic: 0.52\n",
      "___topic 3___\n",
      "lightnes: 0.07\n",
      "sun: 0.08\n",
      "witchcraft: 0.10\n",
      "grey: 0.10\n",
      "magic: 0.15\n",
      "evil: 0.18\n",
      "hue: 0.18\n",
      "pigment: 0.20\n",
      "night: 0.26\n",
      "darknes: 0.30\n",
      "___topic 4___\n",
      "rugby: 0.09\n",
      "pigment: 0.09\n",
      "dead: 0.09\n",
      "grey: 0.09\n",
      "dres: 0.10\n",
      "underworld: 0.11\n",
      "mourning: 0.12\n",
      "wear: 0.15\n",
      "mourn: 0.32\n",
      "death: 0.40\n",
      "\n",
      "\n",
      "LSA Analysis for white documents:\n",
      "___topic 0___\n",
      "snow: 0.09\n",
      "greek: 0.10\n",
      "design: 0.10\n",
      "architect: 0.12\n",
      "column: 0.14\n",
      "roman: 0.15\n",
      "style: 0.17\n",
      "architecture: 0.23\n",
      "building: 0.25\n",
      "temple: 0.38\n",
      "___topic 1___\n",
      "snow: 0.10\n",
      "visible: 0.12\n",
      "illuminant: 0.12\n",
      "spectral: 0.12\n",
      "ray: 0.12\n",
      "primarie: 0.14\n",
      "hue: 0.14\n",
      "opsin: 0.18\n",
      "spectrum: 0.18\n",
      "wavelength: 0.21\n",
      "___topic 2___\n",
      "ray: 0.09\n",
      "illuminant: 0.10\n",
      "nao: 0.10\n",
      "roman: 0.11\n",
      "hue: 0.12\n",
      "spectrum: 0.14\n",
      "opsin: 0.15\n",
      "wavelength: 0.16\n",
      "column: 0.18\n",
      "temple: 0.45\n",
      "___topic 3___\n",
      "wavelength: 0.08\n",
      "design: 0.10\n",
      "primarie: 0.10\n",
      "illuminant: 0.10\n",
      "neoclassical: 0.10\n",
      "hue: 0.11\n",
      "architect: 0.12\n",
      "building: 0.15\n",
      "style: 0.17\n",
      "architecture: 0.22\n",
      "___topic 4___\n",
      "infrared: 0.11\n",
      "gamma: 0.11\n",
      "wavelength: 0.14\n",
      "radiation: 0.14\n",
      "electromagnetic: 0.15\n",
      "visible: 0.16\n",
      "wave: 0.16\n",
      "ray: 0.19\n",
      "spectrum: 0.23\n",
      "opsin: 0.27\n",
      "\n",
      "\n",
      "LSA Analysis for pink documents:\n",
      "___topic 0___\n",
      "association: 0.10\n",
      "rose: 0.11\n",
      "study: 0.12\n",
      "child: 0.12\n",
      "flower: 0.13\n",
      "preference: 0.13\n",
      "boy: 0.19\n",
      "girl: 0.20\n",
      "toy: 0.25\n",
      "gender: 0.29\n",
      "___topic 1___\n",
      "rgb: 0.10\n",
      "specie: 0.11\n",
      "angle: 0.12\n",
      "blossom: 0.12\n",
      "rosa: 0.12\n",
      "plant: 0.17\n",
      "flower: 0.25\n",
      "rise: 0.28\n",
      "magenta: 0.30\n",
      "rose: 0.32\n",
      "___topic 2___\n",
      "inter: 0.13\n",
      "stadium: 0.13\n",
      "football: 0.13\n",
      "season: 0.14\n",
      "cerezo: 0.21\n",
      "league: 0.23\n",
      "osaka: 0.23\n",
      "miami: 0.29\n",
      "palermo: 0.35\n",
      "club: 0.51\n",
      "___topic 3___\n",
      "warda: 0.10\n",
      "wheel: 0.10\n",
      "web: 0.10\n",
      "degree: 0.11\n",
      "rgb: 0.12\n",
      "rise: 0.13\n",
      "angle: 0.14\n",
      "hue: 0.14\n",
      "displayed: 0.14\n",
      "magenta: 0.45\n",
      "___topic 4___\n",
      "bratz: 0.04\n",
      "flower: 0.05\n",
      "release: 0.05\n",
      "puzzle: 0.06\n",
      "plant: 0.07\n",
      "child: 0.07\n",
      "mattel: 0.23\n",
      "toy: 0.37\n",
      "doll: 0.43\n",
      "barbie: 0.52\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#download a library of English stop words and the semantic word database.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function originally from: https://www.programcreek.com/python/?CodeExample=get%20wordnet%20pos\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    # Return the tag, if the tag is not found return noun.\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def load_text_documents(folder_path):\n",
    "    document_texts = []\n",
    "    document_labels = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                #added the stemming function from Week 3 to address the issue of singular and plural forms being counted as separate words in the final results\n",
    "                def simple_stemmer(word):\n",
    "                     stemming_rules = [(r's\\b', '')]\n",
    "                     for pattern, replacement in stemming_rules:\n",
    "                          word = re.sub(pattern, replacement, word)\n",
    "                     return word\n",
    "\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                lemmitized_text = \" \".join([lemmatizer.lemmatize(simple_stemmer(word), get_wordnet_pos(word)) for word in text.split()])\n",
    "                document_texts.append(lemmitized_text)\n",
    "                document_labels.append(os.path.basename(file[:-4]))\n",
    "    \n",
    "    return document_texts, document_labels\n",
    "\n",
    "# an lsa_analysis function is defined to perform LSA topic modeling analysis on folders of different colors separately\n",
    "def lsa_analysis(folder_path, num_topics=5, num_terms=10):\n",
    "    document_texts, document_labels = load_text_documents(folder_path)\n",
    "    #adjust wikipedia_text_stop_words iteratively, eliminating high-frequency words that offer little value in topic analysis\n",
    "    english_stop_words = stopwords.words('english')\n",
    "    wikipedia_text_stop_words = [\n",
    "    'refer', 'may', 'often', 'also', 'refers', 'one', 'use', 'set', 'thus', 'include', 'game','flag','ir','could','century','example','match','large','call',\n",
    "    'make', 'th', 'bc', 'nm', 'many', 'cause', 'well', 'form', 'first', 'well', 'light','name','team','colour','become','since',\n",
    "    'ipl', 'two', 'afl', 'final', 'primary', 'led', 'arc', 'leds', 'around', 'de', 'time','people','year','ex','range','play','sir','state','century',\n",
    "    'ft', 'found', 'von','ha','wa','thi','hi','early','day','new','la','au','work','ga','color','art','history','world','nm','bce','th','red','orange','yellow','green','cyan','blue','purple','black','white','pink']\n",
    "    stop_words = english_stop_words+ wikipedia_text_stop_words\n",
    "\n",
    "    #running previous code resulted in numerous numbers, so I added a step to remove numerical interference. as Reference URL: https://stackoverflow.com/questions/12851791/removing-numbers-from-string\n",
    "    document_texts = [''.join([i for i in text if not i.isdigit()]) for text in document_texts]\n",
    "\n",
    "    #use the `CountVectorizer` class to get our bag of words features for each document\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))\n",
    "    tf_idf = vectorizer.fit_transform(document_texts)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    #convert the TF-IDF vectors into a pandas DataFrame for further analysis.\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=vocab, index=document_labels)\n",
    "    tfidf_df\n",
    "\n",
    "    #create a list of labels, each corresponding to a topic in the LSA model.\n",
    "    labels = ['topic{}'.format(i) for i in range(num_topics)]\n",
    "\n",
    "    svd = TruncatedSVD(n_components = num_topics, n_iter = 100) #You can change n_iter: Higher numbers will take longer but may (or may not) give you better results\n",
    "    svd_topic_vectors = svd.fit_transform(tfidf_df.values)\n",
    "\n",
    "    topic_weights = pd.DataFrame(svd.components_.T, index=vocab, columns=labels)\n",
    "    topic_weights.sample(20)\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        print(\"___topic \" + str(i) + \"___\")\n",
    "        topicName = \"topic\" + str(i)\n",
    "        #sort the topic weights and get the top terms for each topic\n",
    "        weightedlist = topic_weights.get(topicName).sort_values()[-num_terms:]\n",
    "        #print not only the top 10 most important keywords but also output their weights, rounded to 2 decimal places.\n",
    "        for word, weight in weightedlist.items():\n",
    "            print(f\"{word}: {weight:.2f}\")\n",
    "    \n",
    "#list of colors to analyze\n",
    "colors = [\"red\",\"orange\",\"yellow\",\"green\",\"cyan\",\"blue\",\"purple\",\"black\",\"white\",\"pink\"] \n",
    "#directory containing the data\n",
    "data_folder = \"/Users/yinshuodi/Desktop/mini-project-23006440/Webcrawling/code&data/data/my-data\"\n",
    "\n",
    "#iterate through each color and perform LSA analysis\n",
    "for color in colors:\n",
    "    print(f\"LSA Analysis for {color} documents:\")\n",
    "    #construct the path to the folder for the current color\n",
    "    folder_path = os.path.join(data_folder, color)\n",
    "    #perform LSA analysis on the documents in the current folder\n",
    "    lsa_analysis(folder_path)\n",
    "    #print a newline for better readability\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
